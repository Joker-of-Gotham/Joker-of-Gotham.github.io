---
layout: post
title: "机器学习与算法交易(六): 深度学习技术及其在算法交易的应用"
date: 2025-08-18
category: 金融分析
categories: [人工智能, 机器学习]
tags: [综述笔记, 金融量化]
published: true
mermaid: true
math: true
toc: true
---

# 用于交易的深度学习

## 作为表征学习的深度学习

许多⼈⼯智能任务，如图像或语⾳识别，都需要关于世界的知识。其中⼀个关键挑战是如何对这些知识进⾏编码，以便计算机能够利⽤它们。⼏⼗年来，开发机器学习系统需要⼤量的领域专业知识，才能将原始数据（如图像像素）转换为学习算法可以⽤来检测或分类模式的内部表征。

同样，机器学习算法能为交易策略增加多少价值，很⼤程度上取决于我们构建特征的能⼒，这些特征需要能够代表数据中的预测性信息，以便算法能够处理它们。理想情况下，这些特征应能捕捉结果的独⽴驱动因素，正如我们在第四章“⾦融特征⼯程——如何研究阿尔法因⼦”中以及在第⼆部分和第三部分设计和评估捕捉交易信号的因⼦时所讨论的那样。

表示学习（representation learning）允许机器学习算法⾃动发现对检测或分类模式最有⽤的数据表示，⽽不是依赖于⼿动设计的特征。深度学习（DL）将这种技术与关于特征性质的特定假设相结合。

## 深度学习与机器学习和⼈⼯智能的关系

⼈⼯智能的最初⽬标是实现通⽤⼈⼯智能，即解决那些被认为需要⼈类⽔平智能的问题的能⼒，以及对世界进⾏推理和得出逻辑结论并⾃动进⾏⾃我完善的能⼒。不涉及机器学习的⼈⼯智能应⽤包括对世界信息进⾏编码的知识库，并结合⽤于逻辑运算的语⾔。

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/AI线.png" alt="描述文字" width="880" height="480">
</div>

# 用于金融时间序列和卫星图像的CNN

全连接前馈神经⽹络不对输⼊数据的局部结构做任何假设，因此任意重新排序特征对训练结果没有影响。相⽐之下，CNN 的⼀个关键假设是数据具有⽹格状拓扑结构，并且局部结构⾄关重要。换句话说，它们编码了⼀个假设，即输⼊具有图像数据中常⻅的结构：像素形成⼀个⼆维⽹格，可能还有多个通道来表示颜⾊信号的分量。此外，相邻像素的值对于检测边缘和⻆落等关键特征，可能⽐远处的像素点更具相关性。因此，CNN 最初的应⽤⾃然⽽然地集中在图像数据上，例如⼿写识别。

然⽽，随着时间的推移，研究⼈员在时间序列数据中也发现了类似的特征，从⽽拓宽了 CNN 的有效应⽤范围。时间序列数据由按固定间隔进⾏的测量组成，这些测量沿时间轴创建了⼀个⼀维⽹格，例如某个股票代码的滞后收益率。此外，还可以有第⼆个维度，包含该股票代码在同⼀时间段内的其他特征。最后，我们可以⽤第三个维度来表示更多的股票代码。

除了图像之外，CNN 的⼀个常⻅⽤例还包括⾳频数据，既可以是在时域中的⼀维波形，也可以是在经过傅⾥叶变换后，在频域中的⼆维频谱。CNN 在 AlphaGo 中也扮演了关键⻆⾊，这是第⼀个在围棋⽐赛中战胜⼈类的算法，它在⽹格状的棋盘上评估不同的棋局位置。

## CNN原理

### 从⼿动编码到从数据中学习滤波器

对于图像数据，这种局部结构在传统上推动了⼿动编码滤波器的发展，这些滤波器可以提取此类模式，⽤作机器学习(ML)模型中的特征。相⽐之下，卷积层旨在从数据中学习此类局部特征表示。⼀个关键的洞⻅是将其输⼊（称为感受野）限制在输⼊的⼀个⼩区域内，以便捕捉反映边缘或⻆落等常⻅模式的基本像素组合。然⽽，这类模式可能出现在图像的任何位置，因此 CNN 也需要能够识别不同位置的相似模式，并可能允许微⼩的变化。随后的层级则学习合成这些局部特征，以检测更⾼阶的特征。

### 卷积各元素的运作方式

卷积层整合了三种架构思想，使其能够学习到在⼀定程度上对位移、尺度变化和扭曲保持不变的特征表示：

- 稀疏连接（⽽⾮密集连接）
- 权重共享
- 空间或时间降采样

此外，卷积层允许输⼊可变⼤⼩的数据。连续的计算通过卷积、检测器和池化这⼏个阶段来处理输⼊，最先进的 CNN 由多个⼤⼩各异的此类层组成，这些层要么相互堆叠，要么在不同的分⽀上并⾏运⾏。每增加⼀层，⽹络就能检测到更⾼级、更抽象的特征。宽度轴也可以代表时间，⾼度轴代表不同的特征，⽽通道则可以捕捉对不同对象（如股票代码）的观察结果。

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/CNN.png" alt="描述文字" width="880" height="380">
</div>

#### 卷积阶段：提取局部特征

第⼀阶段将⼀个滤波器（也称为核）应⽤于输⼊图像的重叠区域。该滤波器是⼀个矩阵，其尺⼨远⼩于输⼊，因此其感受野仅限于少数连续的值，例如像素或时间序列值。因此，它专注于局部模式，并相对于全连接层显著减少了参数和计算量。⼀个完整的卷积层包含多个特征图，这些特征图以深度切⽚的形式组织起来，以便每⼀层都能提取多种特征。

在扫描输⼊时，卷积核会与其感受野覆盖的每个输⼊⽚段进⾏卷积。卷积操作就是将滤波器权重和匹配的输⼊区域的值分别重塑为向量后，计算它们的点积。因此，每次卷积都会产⽣⼀个数字，⽽整个扫描过程则会⽣成⼀个特征图。由于相同向量的点积最⼤，因此特征图反映了每个输⼊区域的激活程度。

步幅（stride）定义了扫描输⼊时使⽤的步⻓，即⽔平和垂直⽅向上移动的像素数。较⼩的步幅会扫描更多（重叠的）区域，但计算成本也更⾼。当滤波器与输⼊不完全匹配，并在扫描过程中部分越过图像边界时，通常有四种处理⽅式：

- **有效卷积**：丢弃图像和滤波器⽆法完全匹配的扫描结果
- **相同卷积**：对输⼊进⾏零填充，以⽣成⼤⼩相同的特征图
- **完全卷积**：对输⼊进⾏零填充，使得每个像素被扫描的次数相同，包括边界像素（以避免对更靠近中⼼的像素进⾏过采样）
- **因果卷积**：仅在左侧对输⼊进⾏零填充，以使输出不依赖于未来时期的输⼊；保持时间序列数据的时间顺序

#### 检测器阶段：添加⾮线性

特征图通常会经过⾮线性变换。如 softplus 函数：

$$f(x)=ln(1+e^x)$$

#### 池化阶段：对特征图进⾏降采样

卷积层的最后⼀个阶段可能会对特征图的输⼊表示进⾏降采样，以实现以下⽬的：

- 降低其维度并防⽌过拟合
- 降低计算成本
- 实现基本的平移不变性

这假设特征的精确位置不仅对于识别模式不那么重要，甚⾄可能是有害的，因为它很可能在⽬标的不同实例中发⽣变化。池化降低了特征图的空间分辨率，这是⼀种使位置信息不那么精确的简单⽅法。然⽽，这⼀步是可选的，许多架构仅在某些层使⽤池化，或者根本不使⽤。

## 迁移学习：⽤更少的数据实现更快的训练

迁移学习可以带来更好的性能和更快的训练速度，并且需要更少的标记数据。CNN 的迁移学习⽅法依赖于在像 ImageNet 这样的⼤型数据集上进⾏预训练。其⽬标是让卷积滤波器提取出能够泛化到新图像的特征表示。第⼆步，它利⽤预训练的结果来初始化并重新训练⼀个新的 CNN，或者将其作为输⼊，送⼊⼀个处理⽬标任务的新⽹络。

在迁移学习中，常见做法有两类：

- **特征提取（feature extraction）**：将预训练网络的卷积部分参数冻结，仅替换并训练原网络中的全连接部分，或者将瓶颈特征作为输入送入一个新的全连接网络。这样可以利用预训练网络学到的通用低阶特征（如边缘、色块等），同时在新的任务上快速收敛。以 AlexNet 为例，其瓶颈层会为每张输入图像产生一个 4096 维的向量，该向量也可以作为其它机器学习模型（如 SVM、随机森林）的特征输入。
- **微调（fine-tuning）**：在特征提取的基础上，继续训练预训练网络的部分卷积层或全部层以适配新任务。常见策略是冻结前面若干层（保留较通用的低阶特征）而只微调靠近输出端的中高层；或者在数据量足够大时对整个网络进行微调。微调通常能在保持通用表征的同时，使网络后端学习到与目标任务更贴合的高阶特征，但需更小的学习率与更谨慎的正则化以防止过拟合或破坏先验特征。

综上，迁移学习可在“冻结卷积层 + 训练新密集层（或替换分类器）”与“在此基础上微调部分/全部卷积层”两种策略间权衡：前者训练速度快、对小样本稳健；后者在样本充分或任务差异较大时通常能获得更好的性能。

## CNN的具体实践

### 利⽤迁移学习对卫星图像进⾏分类

卫星图像在另类数据中占有重要地位（参⻅第 3 章，⾦融另类数据——类别与⽤例）。例如，⼤宗商品交易员可以依靠卫星图像，通过监测农场活动、矿区动态或油轮运输情况来预测某些农作物或资源的供应量。

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/卫星数据.png" alt="描述文字" width="880" height="400">
</div>

### ⽤于时序数据的 CNN：预测收益率

CNN 已被成功应⽤于⼀维、⼆维和三维的时间数据表示。如果数据满⾜模型的⼀个关键假设，即局部模式或关系有助于预测结果，那么将 CNN 应⽤于时间序列最有可能取得成果。在时间序列的背景下，局部模式可以是相关区间内的⾃相关或类似的⾮线性关系。在第⼆和第三维度上，局部模式意味着多变量序列的不同分量之间，或不同股票代码的这些序列之间存在系统性关系。由于局部性很重要，因此相应地组织数据⾄关重要，这与前馈⽹络不同，在前馈⽹络中，打乱任何维度的元素都不会对学习过程产⽣负⾯影响。

# ⽤于多元时间序列和情感分析的 RNN

## RNN的原理

前馈神经⽹络（FFNN）将每个样本的特征向量视为独⽴同分布。因此，它们在评估当前观测值时不会考虑先前的数据点。换句话说，它们没有记忆。CNN 使⽤的⼀维和⼆维卷积滤波器可以提取特征，这些特征通常是少量相邻数据点的函数。然⽽，它们只允许浅层的参数共享：每个输出都是将相同的滤波器应⽤于相关时间步和特征的结果。RNN 模型的主要创新在于，RNN 允许更复杂的输⼊输出关系，每个输出都是前⼀个输出和新信息的函数。因此，RNN 可以在使⽤当前特征向量进⾏计算时，将先前观测值的信息纳⼊其中。

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/RNN1.png" alt="描述文字" width="880" height="420">
</div>

RNN的输入输出关系一般包括：

- **⼀对多**：例如，图像字幕⽣成，它接收⼀个像素向量（如前⼀章所述），并将其映射为⼀个单词序列
- **多对⼀**：情感分析接收⼀个单词或词元（token）序列（参⻅第 14 章《⽤于交易的⽂本数据——情感分析》），并将其映射为⼀个标量或向量输出
- **多对多**：机器翻译或视频帧标注将输⼊向量序列映射到输出向量序列，可以是同步⽅式（如图所示），也可以是异步⽅式。多变量时间序列的多步预测也是将多个输⼊向量映射到多个输出向量

### 展开一个带循环的计算图

RNN中相邻两层的关系可表示为：

$$y_t=g(W_{hh}h_{t-1}+W_{xh}x_{t})$$

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/RNN2.png" alt="描述文字" width="880" height="360">
</div>

### LSTM 架构的 RNN

⼀个典型的 LSTM 单元包含四个参数化层，这些层通过转换和传递向量来相互作⽤并与单元状态交互。这些层通常包括⼀个输⼊⻔、⼀个输出⻔和⼀个遗忘⻔，但也存在⼀些变体，可能包含额外的⻔或缺少某些机制。

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/RNN3.png" alt="描述文字" width="880" height="370">
</div>

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/RNN4.png" alt="描述文字" width="880" height="370">
</div>

