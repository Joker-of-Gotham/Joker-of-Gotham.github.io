---
layout: post
title: "机器学习与算法交易(五): 无监督学习及相关技术应用"
date: 2025-08-18
category: 金融分析
categories: [人工智能, 机器学习]
tags: [综述笔记, 金融量化]
published: true
mermaid: true
math: true
toc: true
---

# ⽆监督学习实现数据驱动的⻛险因⼦和资产配置

⽆监督学习通过学习⼀种信息丰富的数据表示，帮助探索新数据、发现有⽤的⻅解或更有效地解决其他任务。降维和聚类是⽆监督学习的主要任务。

## 维度灾难

由距离公式：

$$d(p,q)=\sqrt{\sum\limits_{i=1}^{n}(p_i-q_i)^{2}}$$

随特征数量增多，维度更高，特征空间更稀疏，因此需要更多观测值将数据点间的平均距离维持在可接受的范围内。

当观测值之间的距离增⼤时，监督式机器学习会变得更加困难，因为对新样本的预测不太可能基于从相似训练特征中学习到的知识。简⽽⾔之，随着特征数量的增加，可能的唯⼀⾏数呈指数级增⻓，这使得有效采样该空间变得更加困难。

## 线性降维：主成分分析(PCA)与独立成分分析(ICA)

### 主成分分析(PCA)

PCA 旨在捕获数据中的⼤部分⽅差，以便轻松恢复原始特征，并确保每个成分都能增加信息量；它通过将原始数据投影到主成分空间来降低维度。其中维度数由该方法中的超参数决定。

#### PCA的工作原理

PCA 算法的⼯作原理是识别⼀系列成分，在考虑了先前计算的成分所捕获的变异后，每个成分都与数据中最⼤⽅差的⽅向对⻬。这种顺序优化确保了新成分与现有成分不相关，并为向量空间⽣成⼀个正交基。可以简单理解为，不断寻找与已生成主成分向量正交(“垂直”)的向量。

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/PCA.png" alt="描述文字" width="880" height="480">
</div>

该算法寻找向量来创建⼀个⽬标维度的超平⾯，以最⼩化重构误差，该误差以数据点到平⾯的平⽅距离之和来衡量。这个⽬标等同于寻找⼀系列向量，这些向量在给定其他分量的情况下，与最⼤保留⽅差的⽅向对⻬，同时确保所有主成分相互正交。在实践中，该算法通过计算协⽅差矩阵的特征向量或使⽤奇异值分解（SVD）来解决这个问题。

针对数据矩阵$X$，作去中心化：

$$\tilde{X}=X-\bf{1}\mu$$

进而得到协方差矩阵：

$$C=\frac{1}{n}\tilde{X}^{T}\tilde{X}$$

该部分可以与SVD作进一步关联：

$$\tilde{X}=U\Sigma V^{T} \to C=\frac{1}{n}\tilde{X}^{T}\tilde{X}=\frac{1}{n}V \Sigma^{2} V^{T}$$

V的列向量就是协方差矩阵的特征向量，即主成分方向；$\frac{1}{n}\Sigma^{2}$的对角线元素就是协方差矩阵的特征值，对应每个主成分的方差大小。该方差衡量了数据点在对应方向上的离散程度，方差越大，离散程度越大，说明该主成分方向越能有效区分样本。

### 独立成分分析(ICA)

ICA初始时旨在解决盲源分离问题(也被称为“鸡尾酒会问题”)，其目标是从混杂数据中解析出多个独立信号。其数学原理式为：

$$X=AS,S为各独立信号源信号，X为观测信号$$

ICA作出的关键假设包括：信号源在统计上独立，线性变换足以捕获相关信息，独立成分不服从正态分布，混合矩阵A可逆。ICA在实现时还需要把数据均值移到 0（中心化），再把协方差矩阵变成单位矩阵（白化），减少冗余。

## 流形学习：非线性降维

流形假设强调⾼维数据通常位于嵌⼊在⾼维空间中的低维⾮线性流形上或其附近。流形学习旨在找到具有内在维度的流形，然后在这个⼦空间中表示数据。⼀个简化的例⼦是将道路⽤作三维空间中的⼀维流形，并使⽤⻔牌号作为局部坐标来识别数据点。

有⼏种技术可以近似⼀个低维流形。其中⼀个例⼦是局部线性嵌⼊(LLE)，对于每个数据点，LLE 会识别给定数量的最近邻，并计算权重，将每个点表示为其邻居的线性组合。它通过将每个邻域线性投影到低维流形上的全局内部坐标来找到⼀个低维嵌⼊，这可以被看作是⼀系列 PCA 的应⽤。

### t-分布随机邻域嵌⼊(t-SNE)

该算法采⽤⼀种概率性的⾮线性⽅法，将数据定位在⼏个不同但相关的低维流形上，强调在低维空间中将相似的点聚集在⼀起，⽽不是像 PCA 等最⼩化平⽅距离的算法那样，维持⾼维空间中相距较远的点之间的距离。

该算法的流程是先将⾼维距离转换为（条件）概率，其中⾼概率意味着低距离，并反映了基于相似性对两个点进⾏采样的可能性。为实现这⼀点，它⾸先在每个点上放置⼀个正态分布，并计算该点及其每个邻居的密度，其中“困惑度”参数控制着有效邻居的数量。第⼆步，它将点排列在低维空间中，并使⽤类似计算出的低维概率来匹配⾼维分布。它使⽤ Kullback-Leibler 散度来衡量分布之间的差异，这种⽅法会对在低维空间中错误放置相似点的情况施加⾼额惩罚。

遗憾的是，t-SNE 不⽀持将新的数据点投影到低维空间。由于 t-SNE 对⼩距离和⼤距离的处理⽅式不同，其压缩后的输出对于基于距离或密度的聚类算法来说并不是⼀个⾮常有⽤的输⼊。

### 均匀流形近似与投影

UMAP 是⼀种较新的算法，⽤于可视化和通⽤的降维。它假设数据均匀分布在⼀个局部连接的流形上，并利⽤模糊拓扑学来寻找最接近的低维等价物。它使⽤⼀个 `neighbors` 参数，其对结果的影响与前⼀节中的困惑度（perplexity）类似。它⽐ t-SNE 更快，因此能更好地扩展到⼤型数据集，并且有时⽐ t-SNE 更好地保留全局结构。它还可以使⽤不同的距离函数，包括⽤于衡量词频向量之间距离的余弦相似度。

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/流形学习.png" alt="描述文字" width="880" height="480">
</div>

## 聚类

聚类和降维都是对数据进⾏总结的⽅法。正如我们刚刚讨论的，降维通过使⽤更少的新特征来表示数据，从⽽压缩数据，这些新特征捕捉了最相关的信息。相⽐之下，聚类算法则是将现有的观测数据分配到由相似数据点组成的⼦组中。

### K-Means聚类

该方法通过随机定义K个簇心，并将数据点分配到最近的质心，直到分配结果（或簇内变异）不再改变为止。该方法的效果可以用下述得分函数衡量：

$$s=\frac{b-a}{\max(a,b)} \in [-1,1] $$

### 层次聚类

层次聚类⽆需指定⽬标聚类数量，因为它假定数据可以被连续地合并到差异越来越⼤的聚类中。它不追求全局⽬标，⽽是逐步决定如何⽣成⼀系列嵌套的聚类，其范围可以从单个聚类到由单个数据点组成的聚类。

层次聚类有两种⽅法：

- 凝聚聚类采⽤⾃下⽽上的⽅式，根据相似性依次合并剩余的两个组
- 分裂聚类采⽤⾃上⽽下的⽅式，依次分割剩余的簇，以产⽣最独特的⼦组

#### 凝聚聚类算法

凝聚聚类算法从单个数据点出发，计算⼀个包含所有相互距离的相似度矩阵。然后他执行$N-1$个步骤，直到没有更多独立簇为止。衡量簇间（⽽⾮单个数据点之间）相异性的⽅法对聚类结果有重要影响。各种选项的区别如下：

- **单链接法（Single-link）**：两个簇中最近邻居之间的距离
- **完全连接**：相应聚类成员之间的最⼤距离
- **沃德法（Ward’s method）**：最⼩化簇内⽅差
- **组平均法**：使⽤簇的中点作为参考距离

#### 可视化：树状图

层次聚类通过持续合并数据，深⼊揭示了观测值之间的相似程度。当⼀次合并与下⼀次合并的相似性度量发⽣显著变化时，这表明在此之前存在⼀个⾃然的聚类。不同的连接⽅法会产⽣外观不同的树状图，因此我们⽆法通过这种可视化⽅式来⽐较不同⽅法的结果。

层次聚类的优点包括：

- 不需要指定具体的簇数，⽽是通过直观的可视化⽅式，为潜在的聚类提供洞⻅
- 能⽣成⼀个层次化的簇结构，可作为分类体系使⽤
- 可以与 k-means 算法结合，以减少凝聚过程开始时的项⽬数量

层次聚类的缺点包括：

- 需要频繁更新相似度矩阵，计算和内存成本很⾼
- 所有的合并都是最终的，因此⽆法达到全局最优
- 维度灾难导致难以处理嘈杂的⾼维数据

### 基于密度的聚类

基于密度的聚类算法根据与其他聚类成员的邻近度来分配聚类成员资格。其⽬标是识别任意形状和⼤⼩的密集区域。这类算法不需要指定聚类的数量，⽽是依赖于定义邻域⼤⼩和密度阈值的参数。

#### 基于噪声应⽤的密度聚类算法(DBSCAN)

该算法旨在识别核⼼样本和⾮核⼼样本，前者⽤于扩展簇，后者属于某个簇但其附近没有⾜够的邻居来进⼀步扩⼤该簇。它使⽤参数 eps 表示邻域半径，使⽤ min_samples 表示核⼼样本所需的成员数量。该算法是确定性的、排他性的，并且在处理不同密度的簇和⾼维数据时会遇到困难。将参数调整到所需的密度可能具有挑战性，尤其是当密度通常不恒定时。

#### 层次DBSCAN

层次聚类DBSCAN(HDBSCAN)是⼀项较新的发展，它假设簇是密度可能不同的孤岛，以克服前⾯提到的 DBSCAN 的挑战。

### ⾼斯混合模型(GMM)

⾼斯混合模型（GMM）是⼀种⽣成模型，它假设数据是由多种多元正态分布混合⽣成的。该算法旨在估计这些分布的均值和协⽅差矩阵。⾼斯混合模型是对 k-means 算法的泛化：它增加了特征之间的协⽅差，使得聚类可以是椭球形⽽⾮球形，⽽质⼼则由每个分布的均值表示。⾼斯混合模型算法执⾏的是软分配，因为每个数据点都有可能属于任何⼀个聚类。

## 层次⻛险平价(HRP)

层次化⻛险平价的关键思想为：利⽤协⽅差矩阵的层次化聚类，将具有相似相关性结构的资产分组；在构建投资组合时，仅将相似资产视为替代品，从⽽减少⾃由度。

# ⽤于交易情绪分析的⽂本数据

在本章中，我们将介绍⼀些基本的特征提取技术，这些技术侧重于单个语义单元，即单词或称为“词元”（token）的短词组。我们将展示如何通过创建⽂档-词项矩阵（document-term matrix）将⽂档表示为词元计数的向量，然后将其⽤作新闻分类和情感分析的输⼊。我们还将介绍常⽤于此⽬的的朴素⻉叶斯算法。

## 处理⽂本数据的主要挑战

⾃然语⾔处理（NLP）尤其具有挑战性，因为要将⽂本数据有效地⽤于机器学习，既需要理解语⾔的内在运作机制，也需要
了解其所指代的现实世界知识。主要挑战包括：

- 由⼀词多义引起的歧义，即⼀个词或短语在不同语境下有不同含义
- 语⾔的⾮标准和不断演变的⽤法，尤其是在社交媒体上
- 习语的使⽤，例如“throw in the towel”（意为“认输”）
- 棘⼿的实体名称，例如《“ ⾍⾍危机》在哪⾥上映？”
- 世界知识，例如“玛丽和苏是姐妹”与“玛丽和苏是⺟亲”

## ⾃然语⾔处理⼯作流

利⽤⽂本数据进⾏机器学习以实现算法交易的⼀个关键⽬标，是从⽂档中提取信号。⽂档是来⾃相关⽂本数据源的单个样本，例如公司报告、新闻标题、新闻⽂章或推⽂。⽽语料库则是⽂档的集合。

基础技术将⽂本特征提取为称为“词元”（token）的独⽴语义单元，并使⽤规则和词典为其标注语⾔和语义信息。词袋模型使⽤词元频率将⽂档建模为词元向量，从⽽⽣成⽂档-词条矩阵，该矩阵常⽤于⽂本分类、检索或摘要。更⾼级的⽅法则依赖机器学习来优化词元等基本特征，从⽽⽣成更丰富的⽂档模型。这些⽅法包括反映词元在不同⽂档中联合使⽤情况的主题模型，以及旨在捕捉词元使⽤语境的词向量模型。

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/NLP流.png" alt="描述文字" width="880" height="140">
</div>

## ⽤于交易的⾃然语⾔处理

### 朴素⻉叶斯分类器

朴素⻉叶斯算法在⽂本分类领域⾮常流⾏，因为它计算成本低、内存需求少，便于在⾮常庞⼤、⾼维的数据集上进⾏训练。它的预测性能可以与更复杂的模型相媲美，提供了⼀个很好的基准，并以其在垃圾邮件检测⽅⾯的成功⽽闻名。

该模型依赖于⻉叶斯定理以及⼀个假设，即在给定结果类别的情况下，各个特征之间是相互独⽴的。换句话说，对于⼀个给定的结果，知道⼀个特征的值（例如，⽂档中某个词元的存在）并不能提供关于另⼀个特征值的任何信息。

# 主题建模总结⾦融新闻

本章中，我们将使⽤⽆监督机器学习，通过主题建模从⽂档中提取隐藏的主题。这些主题能够以⾃动化的⽅式，为我们提供对⼤量⽂档的详细洞⻅。它们对于理解“⼤海捞针”中的“草堆”本身⾮常有⽤，并允许我们根据⽂档与不同主题的关联性为其打上标签。

主题模型能够⽣成复杂且可解释的⽂本特征，这可以作为从⼤量⽂档中提取交易信号的第⼀步。它们能加快⽂档审阅速度，帮助识别和聚类相似⽂档，并为预测建模提供⽀持。

## 潜在语义索引(LSI)

LSI旨在改善那些因查询词的同义词⽽遗漏了相关⽂档的查询结果，其⽬标是为⽂档和词条之间的关系建模，从⽽能够预测某个词条应与某篇⽂档相关联，即使由于⽤词的多样性，实际上并未观察到这种关联。

LSI通过分解 DTM 来利⽤线性代数找到给定数量$k$的潜在主题。在此背景下，SVD 识别出⼀组不相关的索引变量或因⼦，通过其因⼦值向量来表示每个词条和⽂档。SVD 如何将 DTM 分解为三个矩阵：两个包含正交奇异向量的矩阵，以及⼀个包含奇异值的对⻆矩阵，这些奇异值⽤作缩放因⼦。DTM 的 LSI 分解可以解释为：

- 第⼀个 $M × T$ 矩阵表示⽂档与主题之间的关系
- 对⻆矩阵根据主题在语料库中的强度对其进⾏缩放
- 第三个矩阵则对词项与主题的关系进⾏建模

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/LSI-SVD.png" alt="描述文字" width="880" height="280">
</div>

## 概率潜在语义分析(pLSA)

概率潜在语义分析（pLSA）从统计学⻆度看待 LSI/LSA，并创建了⼀个⽣成模型，以解决 LSA 缺乏理论基础的问题。该模型有两种认知方式，一种认为词语和文档都由潜在主题生成，一种认为是主题来自于文档而词语来自于主题：

$$P(w,d)=\sum\limits_{t}P(d \| t)P(w \| t)=P(d)\sum\limits_{t}P(t \| d)P(w \| t)$$

## 潜在狄利克雷分配(LDA)

潜在狄利克雷分配（LDA）通过为主题添加⼀个⽣成过程，它是最受欢迎的主题模型，因为它倾向于产⽣⼈类可以理解的有意义的主题，能够为新⽂档分配主题，并且具有可扩展性。LDA 模型的变体可以包含元数据（如作者或图像数据），或者学习层次化主题。

### LDA的工作原理

LDA 是⼀种分层⻉叶斯模型，它假设主题是词语的概率分布，⽽⽂档是主题的概率分布。更具体地说，该模型假设主题遵循稀疏的狄利克雷分布，这意味着⽂档只反映⼀⼩部分主题，⽽主题也只频繁使⽤有限数量的术语。

当作者向⽂档库中添加⼀篇⽂章时，LDA 主题模型会假定存在以下⽣成过程：

- 按照狄利克雷概率定义的⽐例，随机混合⼀⼩部分主题
- 对于⽂本中的每个词，根据⽂档-主题概率选择⼀个主题
- 根据主题-词语概率，从该主题的词语列表中选择⼀个词语

因此，⽂章内容取决于每个主题的权重以及构成每个主题的词语。狄利克雷分布控制着⽂档主题的选择和主题词语的选择。它体现了这样⼀种思想：⼀篇⽂档只涵盖少数⼏个主题，⽽每个主题只频繁使⽤少数⼏个词语。

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/LDA.png" alt="描述文字" width="880" height="480">
</div>

### 评估 LDA 主题

客观地评估结果的两种⽅法为：

- 困惑度 (perplexity)
- 主题⼀致性指标 (topic coherence metrics)

#### 困惑度

困惑度的计算公式如下：

$$2^{h(p)}=2^{-\Sigma_{w} p(w) \log_{2} p(w)}$$

越接近于零的度量值意味着该分布能更好地预测样本。

#### 主题⼀致性

主题⼀致性衡量主题模型结果的语义⼀致性，即⼈类是否认为与主题相关的词语及其概率是有意义的。为此，它通过衡量与主题最相关的词语之间的语义相似度来为每个主题打分。有两种专为 LDA 设计的相⼲性度量⽅法，它们分别是 UMass 和 UCI 指标。

- **UCI 指标**：将⼀个词对的分数定义为两个不同（热⻔）主题词对$w_{i},w_{j} \in W$之间的住店互信息(PMI)与一个平滑因子$\epsilon$的总和

    $$\text{coherence}_{\text{UCI}}=\sum\limits_{(w_i,w_j) \in W} \log \frac{p(w_i,w_j)+\epsilon}{p(w_i) p(w_j)}$$

    这些概率是通过在维基百科等外部语料库上使⽤滑动窗⼝计算词语的共现频率得出的，因此该指标可以被视为与语义“地⾯实况”的外部⽐较。

- **UMass 指标**：使⽤训练语料库中若⼲⽂档$D$内的共现情况来计算相⼲性分数

    $$\text{coherence}_{\text{UMass}}=\sum\limits_{(w_i,w_j) \in W} \log \frac{D(w_i,w_j)+\epsilon}{D(w_j)}$$

    该指标反映的是内在⼀致性，⽽⾮将模型结果与外部的真实情况进⾏⽐较。

# ⽤于财报电话会议和 SEC ⽂件的词嵌⼊

要开发基于⽂本数据的交易策略，我们通常关⼼的是⽂档的含义，⽽不是单个词元。例如，我们可能希望创建⼀个数据集，使⽤代表推⽂或新闻⽂章的特征，并附带情感信息（参⻅第 14 章，⽤于交易的⽂本数据——情感分析），或者在⽂章发布后给定时间范围内的资产回报。尽管词袋模型在编码⽂本数据时会丢失⼤量信息，但它的优点是能够表示整个⽂档。然⽽，词嵌⼊技术已进⼀步发展，能够表示⽐单个词元更多的内容。

## word2vec：可扩展的词和短语嵌⼊

word2vec 模型是⼀个两层神经⽹络，它将⽂本语料库作为输⼊，并为该语料库中的单词输出⼀组嵌⼊向量。其主流架构包括两种：

- 连续词袋模型(CBOW)：使⽤上下⽂词向量的平均值作为输⼊来预测⽬标词，因此词序⽆关紧要。CBOW 训练速度更快，对于⾼频词的准确性也略⾼，但对低频词的关注度较低。
- 跳字模型(skip-gram，SG)：使⽤⽬标词来预测从上下⽂中采样的词。它在⼩型数据集上表现良好，即使对于罕⻅词或短语也能找到很好的表示。

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/词袋两种.png" alt="描述文字" width="880" height="380">
</div>

该模型接收⼀个嵌⼊向量作为输⼊，并计算其与另⼀个嵌⼊向量的点积。训练过程是在⽂档上滑动上下⽂窗⼝来进⾏的，这些⽂档通常被分割成句⼦。对语料库的每⼀次完整迭代称为⼀个周期（epoch）。根据数据的不同，可能需要⼏⼗个周期才能使向量质量收敛。

模型的最后一层用softmax函数作为输出单元实现多分类目标：

$$p(w \| c)=\frac{\exp (h^{T}v_{w}')}{\sum\limits_{w_i \in V}\exp (h^{T}v_{w_i}')} \ (h指嵌入向量，v指输入向量，c是词语w的上下文)$$

然⽽，softmax 的复杂度会随着类别数量的增加⽽增加，因为分⺟需要计算词汇库中所有词语的点积来对概率进⾏标准化。Word2vec 通过使⽤ softmax 的修改版本或基于采样的近似⽅法来提⾼效率，如分层softmax不用对所有词算 softmax，而是把词表组织成一棵二叉树（哈夫曼树）。而采样方法包括：

- 噪声对⽐估计(NCE)：对上下⽂之外的“噪声词”进⾏采样，并通过⼆元分类问题来近似多类别任务。随着样本数量的增加，NCE 的导数会逼近 softmax 的梯度，但仅⽤ 25 个样本就能实现与 softmax 相似的收敛速度，且速度快 45 倍。
- 负采样(NEG)：省略了噪声词样本来近似 NCE，并直接最⼤化⽬标词的概率；因此，NEG 优化的是嵌⼊向量的语义质量（相似⽤法对应相似向量），⽽不是在测试集上的准确率。然⽽，与分层 softmax ⽬标相⽐，它可能对不常⽤词产⽣较差的表示。

### 词向量训练前的短语检测

Word2Vec 或其他词向量模型默认每个词是一个基本单位（token），比如 “New” 和 “York” 会被当作两个词。但是某些词组合经常一起出现，并且意义固定，例如“New York City”。短语检测就是在预处理阶段，把这些高频、固定搭配的词组合成一个“单一 token”。

Mikolov的简单方法为提升度评分：

$$\text{score}(w_1,w_2)=\frac{\text{count}(w_1,w_2)-\delta}{\text{count}(w_1) \cdot \text{count}(w_2)}$$

如果 score 超过阈值，就把 $w_1$ 和 $w_2$ 当作一个 bigram

更复杂的方法为归一化逐点互信息(PMI)：

$$P(w_1,w_2)=\frac{\text{count}(w_1,w_2)}{\text{总词对数}}\ , \ P(w)=\frac{\text{count}(w)}{\text{总词数}} \to \text{PMI}(w_1,w_2)=\log \frac{P(w_1,w_2)}{P(w_1) \cdot P(w_2)}$$

归一化版本把 PMI 压缩到 $[-1, +1]$，更稳定。

而NPMI的计算公式为：

$$NPMI=\frac{ln(\frac{P(w_1,w_2)}{P(w_1) \cdot P(w_2)})}{-ln(P(w_i,w_j))}$$

许多任务需要特定领域的词汇嵌⼊，⽽那些在通⽤语料库上预训练的模型可能⽆法捕捉到这些词汇的含义。标准的word2vec模型⽆法为词汇表之外的词分配向量，⽽是使⽤⼀个默认向量，这会降低其预测价值。