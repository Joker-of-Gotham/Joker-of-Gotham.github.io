---
layout: post
title: "机器学习与算法交易(五): 无监督学习与因子配置提取"
date: 2025-08-18
category: 金融分析
categories: [人工智能, 机器学习]
tags: [综述笔记, 金融量化]
published: true
mermaid: true
math: true
toc: true
---

# ⽆监督学习实现数据驱动的⻛险因⼦和资产配置

⽆监督学习通过学习⼀种信息丰富的数据表示，帮助探索新数据、发现有⽤的⻅解或更有效地解决其他任务。降维和聚类是⽆监督学习的主要任务。

## 维度灾难

由距离公式：

$$d(p,q)=\sqrt{\sum\limits_{i=1}^{n}(p_i-q_i)^{2}}$$

随特征数量增多，维度更高，特征空间更稀疏，因此需要更多观测值将数据点间的平均距离维持在可接受的范围内。

当观测值之间的距离增⼤时，监督式机器学习会变得更加困难，因为对新样本的预测不太可能基于从相似训练特征中学习到的知识。简⽽⾔之，随着特征数量的增加，可能的唯⼀⾏数呈指数级增⻓，这使得有效采样该空间变得更加困难。

## 线性降维：主成分分析(PCA)与独立成分分析(ICA)

### 主成分分析(PCA)

PCA 旨在捕获数据中的⼤部分⽅差，以便轻松恢复原始特征，并确保每个成分都能增加信息量；它通过将原始数据投影到主成分空间来降低维度。其中维度数由该方法中的超参数决定。

#### PCA的工作原理

PCA 算法的⼯作原理是识别⼀系列成分，在考虑了先前计算的成分所捕获的变异后，每个成分都与数据中最⼤⽅差的⽅向对⻬。这种顺序优化确保了新成分与现有成分不相关，并为向量空间⽣成⼀个正交基。可以简单理解为，不断寻找与已生成主成分向量正交(“垂直”)的向量。

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/PCA.png" alt="描述文字" width="880" height="480">
</div>

该算法寻找向量来创建⼀个⽬标维度的超平⾯，以最⼩化重构误差，该误差以数据点到平⾯的平⽅距离之和来衡量。这个⽬标等同于寻找⼀系列向量，这些向量在给定其他分量的情况下，与最⼤保留⽅差的⽅向对⻬，同时确保所有主成分相互正交。在实践中，该算法通过计算协⽅差矩阵的特征向量或使⽤奇异值分解（SVD）来解决这个问题。

针对数据矩阵$X$，作去中心化：

$$\tilde{X}=X-\bf{1}\mu$$

进而得到协方差矩阵：

$$C=\frac{1}{n}\tilde{X}^{T}\tilde{X}$$

该部分可以与SVD作进一步关联：

$$\tilde{X}=U\Sigma V^{T} \to C=\frac{1}{n}\tilde{X}^{T}\tilde{X}=\frac{1}{n}V \Sigma^{2} V^{T}$$

V的列向量就是协方差矩阵的特征向量，即主成分方向；$\frac{1}{n}\Sigma^{2}$的对角线元素就是协方差矩阵的特征值，对应每个主成分的方差大小。该方差衡量了数据点在对应方向上的离散程度，方差越大，离散程度越大，说明该主成分方向越能有效区分样本。

### 独立成分分析(ICA)

ICA初始时旨在解决盲源分离问题(也被称为“鸡尾酒会问题”)，其目标是从混杂数据中解析出多个独立信号。其数学原理式为：

$$X=AS,S为各独立信号源信号，X为观测信号$$

ICA作出的关键假设包括：信号源在统计上独立，线性变换足以捕获相关信息，独立成分不服从正态分布，混合矩阵A可逆。ICA在实现时还需要把数据均值移到 0（中心化），再把协方差矩阵变成单位矩阵（白化），减少冗余。

## 流形学习：非线性降维

流形假设强调⾼维数据通常位于嵌⼊在⾼维空间中的低维⾮线性流形上或其附近。流形学习旨在找到具有内在维度的流形，然后在这个⼦空间中表示数据。⼀个简化的例⼦是将道路⽤作三维空间中的⼀维流形，并使⽤⻔牌号作为局部坐标来识别数据点。

有⼏种技术可以近似⼀个低维流形。其中⼀个例⼦是局部线性嵌⼊(LLE)，对于每个数据点，LLE 会识别给定数量的最近邻，并计算权重，将每个点表示为其邻居的线性组合。它通过将每个邻域线性投影到低维流形上的全局内部坐标来找到⼀个低维嵌⼊，这可以被看作是⼀系列 PCA 的应⽤。

### t-分布随机邻域嵌⼊(t-SNE)

该算法采⽤⼀种概率性的⾮线性⽅法，将数据定位在⼏个不同但相关的低维流形上，强调在低维空间中将相似的点聚集在⼀起，⽽不是像 PCA 等最⼩化平⽅距离的算法那样，维持⾼维空间中相距较远的点之间的距离。

该算法的流程是先将⾼维距离转换为（条件）概率，其中⾼概率意味着低距离，并反映了基于相似性对两个点进⾏采样的可能性。为实现这⼀点，它⾸先在每个点上放置⼀个正态分布，并计算该点及其每个邻居的密度，其中“困惑度”参数控制着有效邻居的数量。第⼆步，它将点排列在低维空间中，并使⽤类似计算出的低维概率来匹配⾼维分布。它使⽤ Kullback-Leibler 散度来衡量分布之间的差异，这种⽅法会对在低维空间中错误放置相似点的情况施加⾼额惩罚。

遗憾的是，t-SNE 不⽀持将新的数据点投影到低维空间。由于 t-SNE 对⼩距离和⼤距离的处理⽅式不同，其压缩后的输出对于基于距离或密度的聚类算法来说并不是⼀个⾮常有⽤的输⼊。

### 均匀流形近似与投影

UMAP 是⼀种较新的算法，⽤于可视化和通⽤的降维。它假设数据均匀分布在⼀个局部连接的流形上，并利⽤模糊拓扑学来寻找最接近的低维等价物。它使⽤⼀个 `neighbors` 参数，其对结果的影响与前⼀节中的困惑度（perplexity）类似。它⽐ t-SNE 更快，因此能更好地扩展到⼤型数据集，并且有时⽐ t-SNE 更好地保留全局结构。它还可以使⽤不同的距离函数，包括⽤于衡量词频向量之间距离的余弦相似度。

<div style="text-align: center;">
<img src="/assets/images/LLM学习/机器学习与金融量化/流形学习.png" alt="描述文字" width="880" height="480">
</div>

## 聚类

聚类和降维都是对数据进⾏总结的⽅法。正如我们刚刚讨论的，降维通过使⽤更少的新特征来表示数据，从⽽压缩数据，这些新特征捕捉了最相关的信息。相⽐之下，聚类算法则是将现有的观测数据分配到由相似数据点组成的⼦组中。

### K-Means聚类

该方法通过随机定义K个簇心，并将数据点分配到最近的质心，直到分配结果（或簇内变异）不再改变为止。该方法的效果可以用下述得分函数衡量：

$$s=\frac{b-a}{\max(a,b)} \in [-1,1] $$

### 层次聚类

层次聚类⽆需指定⽬标聚类数量，因为它假定数据可以被连续地合并到差异越来越⼤的聚类中。它不追求全局⽬标，⽽是逐步决定如何⽣成⼀系列嵌套的聚类，其范围可以从单个聚类到由单个数据点组成的聚类。

层次聚类有两种⽅法：

- 凝聚聚类采⽤⾃下⽽上的⽅式，根据相似性依次合并剩余的两个组
- 分裂聚类采⽤⾃上⽽下的⽅式，依次分割剩余的簇，以产⽣最独特的⼦组

#### 凝聚聚类算法

凝聚聚类算法从单个数据点出发，计算⼀个包含所有相互距离的相似度矩阵。然后他执行$N-1$个步骤，直到没有更多独立簇为止。衡量簇间（⽽⾮单个数据点之间）相异性的⽅法对聚类结果有重要影响。各种选项的区别如下：

- **单链接法（Single-link）**：两个簇中最近邻居之间的距离
- **完全连接**：相应聚类成员之间的最⼤距离
- **沃德法（Ward’s method）**：最⼩化簇内⽅差
- **组平均法**：使⽤簇的中点作为参考距离

#### 可视化：树状图

层次聚类通过持续合并数据，深⼊揭示了观测值之间的相似程度。当⼀次合并与下⼀次合并的相似性度量发⽣显著变化时，这表明在此之前存在⼀个⾃然的聚类。不同的连接⽅法会产⽣外观不同的树状图，因此我们⽆法通过这种可视化⽅式来⽐较不同⽅法的结果。

层次聚类的优点包括：

- 不需要指定具体的簇数，⽽是通过直观的可视化⽅式，为潜在的聚类提供洞⻅
- 能⽣成⼀个层次化的簇结构，可作为分类体系使⽤
- 可以与 k-means 算法结合，以减少凝聚过程开始时的项⽬数量

层次聚类的缺点包括：

- 需要频繁更新相似度矩阵，计算和内存成本很⾼
- 所有的合并都是最终的，因此⽆法达到全局最优
- 维度灾难导致难以处理嘈杂的⾼维数据

### 基于密度的聚类

基于密度的聚类算法根据与其他聚类成员的邻近度来分配聚类成员资格。其⽬标是识别任意形状和⼤⼩的密集区域。这类算法不需要指定聚类的数量，⽽是依赖于定义邻域⼤⼩和密度阈值的参数。

#### 基于噪声应⽤的密度聚类算法(DBSCAN)

该算法旨在识别核⼼样本和⾮核⼼样本，前者⽤于扩展簇，后者属于某个簇但其附近没有⾜够的邻居来进⼀步扩⼤该簇。它使⽤参数 eps 表示邻域半径，使⽤ min_samples 表示核⼼样本所需的成员数量。该算法是确定性的、排他性的，并且在处理不同密度的簇和⾼维数据时会遇到困难。将参数调整到所需的密度可能具有挑战性，尤其是当密度通常不恒定时。

#### 层次DBSCAN

层次聚类DBSCAN(HDBSCAN)是⼀项较新的发展，它假设簇是密度可能不同的孤岛，以克服前⾯提到的 DBSCAN 的挑战。

### ⾼斯混合模型(GMM)

⾼斯混合模型（GMM）是⼀种⽣成模型，它假设数据是由多种多元正态分布混合⽣成的。该算法旨在估计这些分布的均值和协⽅差矩阵。⾼斯混合模型是对 k-means 算法的泛化：它增加了特征之间的协⽅差，使得聚类可以是椭球形⽽⾮球形，⽽质⼼则由每个分布的均值表示。⾼斯混合模型算法执⾏的是软分配，因为每个数据点都有可能属于任何⼀个聚类。

## 层次⻛险平价(HRP)

层次化⻛险平价的关键思想为：利⽤协⽅差矩阵的层次化聚类，将具有相似相关性结构的资产分组；在构建投资组合时，仅将相似资产视为替代品，从⽽减少⾃由度。

# ⽤于交易情绪分析的⽂本数据

